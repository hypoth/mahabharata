{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# import statistics\n",
    "# import json\n",
    "# import tiktoken\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "##\n",
    "load_dotenv('../.env') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model \n",
    "\n",
    "Roberta Named Entity works best for this task. \n",
    "\n",
    "Using this version from HF : `mn-xlm-roberta-base-named-entity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters -> 277.456901 Mn\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "## Roberta based NER\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"2rtl3/mn-xlm-roberta-base-named-entity\")\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(\"2rtl3/mn-xlm-roberta-base-named-entity\")\n",
    "nlp_roberta = pipeline(\"ner\", model=roberta_model, tokenizer=roberta_tokenizer)\n",
    "\n",
    "print(\"Number of parameters ->\", roberta_model.num_parameters()/1000000, \"Mn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOC', 'MISC', 'O', 'ORG', 'PER']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entity_classes = list(roberta_model.config.label2id.keys())\n",
    "entity_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for combining tokens to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words that must not be considered as names\n",
    "stop_words = [\"\", \"the\", \"The\", \"THE\", \"Sir\", \"Dr\", \"Mr\", \"of\", \"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## Helper funciton to combine tokens into names and output a list of objects with entity types and names\n",
    "def combine_tokens_list(ner_results, stop_words = [], metadata={}):\n",
    "    ## ner_results are the results out of the Roberta Named Entity Recognition model. \n",
    "    ## stop_words are the words that should be disccarded as named entities\n",
    "    ## metadata is an optional dictionary of attributes to be added to the output.\n",
    "    ##  This can be used to add section number, chapter number, etc to the output.\n",
    "    name = \"\"\n",
    "    entities_list = []\n",
    "    entity = np.nan\n",
    "    current_word_start = 0\n",
    "    prev_word_end = 0\n",
    "    for res in ner_results:\n",
    "        word = res['word']\n",
    "        current_word_start = res['start']\n",
    "        if (word[0] == \"▁\") or (current_word_start > prev_word_end):\n",
    "            ## Save previous name\n",
    "            if not ( name in stop_words or len(name)<=2):\n",
    "                ## If entity is not set yet, then set current entity.\n",
    "                entity = res['entity'] if entity == \"NA\" else entity\n",
    "                ## removing trailing hypens from the names before saving\n",
    "                entities_list = entities_list + [{'name': name.rstrip(\"-\"), 'entity': entity, **metadata}]\n",
    "            name = re.sub(r'[^a-zA-Z0-9\\-]', '', word)\n",
    "            entity = res['entity']\n",
    "        else:\n",
    "            # Remove all the special characters except '-' from the token\n",
    "            # Add token to the ongoing name. \n",
    "            name = name + re.sub(r'[^a-zA-Z0-9\\-]', '', word)\n",
    "            \n",
    "        prev_word_end = res['end']\n",
    "    \n",
    "    ## append the last name\n",
    "    entities_list = entities_list + [{'name': name, 'entity': entity, **metadata}]\n",
    "    ## Return\n",
    "    return entities_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Helper funciton to combine tokens into names and arrange them into dictionary\n",
    "def combine_tokens_dict(ner_results, classes = [], stop_words = []):\n",
    "    name = \"\"\n",
    "    ## Define a dictionary of entities and add classes as keys in the python dict\n",
    "    entities_dict = {}\n",
    "    for label in classes:\n",
    "        entities_dict[label] = []\n",
    "    ##\n",
    "    entity = \"NA\"\n",
    "    current_word_start = 0\n",
    "    prev_word_end = 0\n",
    "    for res in ner_results:\n",
    "        word = res['word']\n",
    "        current_word_start = res['start']\n",
    "        if (word[0] == \"▁\") or (current_word_start > prev_word_end):\n",
    "            ## Save previous name\n",
    "            if not ( name in stop_words or len(name)<=2):\n",
    "                ## If entity is not set yet, then set current entity.\n",
    "                entity = res['entity'] if entity == \"NA\" else entity\n",
    "                ## removing trailing hypens from the names before saving\n",
    "                entities_dict[entity] = entities_dict[entity] + [name.rstrip(\"-\")]\n",
    "            name = re.sub(r'[^a-zA-Z0-9\\-]', '', word)\n",
    "            entity = res['entity']\n",
    "        else:\n",
    "            # Remove all the special characters except '-' from the token.\n",
    "            # Add token to the ongoing name. \n",
    "            name = name + re.sub(r'[^a-zA-Z0-9\\-]', '', word)\n",
    "            \n",
    "        prev_word_end = res['end']\n",
    "    \n",
    "    ## append the last name\n",
    "    entities_dict[entity] = entities_dict[entity] + [name.rstrip(\"-\")]\n",
    "    ## Calculate unique values per class\n",
    "    for label in classes:\n",
    "        entities_dict[label] = list(set(entities_dict[label]))\n",
    "    return entities_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Named entites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the model for multiple text\n",
    "\n",
    "first lets run a test on example text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Ugrasrava, the son of Lomaharshana, surnamed Sauti, well-versed in the\n",
    "Puranas, bending with humility, one day approached the great sages of\n",
    "rigid vows, sitting at their ease, who had attended the twelve years'\n",
    "sacrifice of Saunaka, surnamed Kulapati, in the forest of Naimisha.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Test the combine_tokens_dict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': ['Naimisha'],\n",
       " 'MISC': ['Puranas'],\n",
       " 'O': [],\n",
       " 'ORG': [],\n",
       " 'PER': ['Sauti', 'Saunaka', 'Kulapati', 'Ugrasrava', 'Lomaharshana']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Calculate named entities as dictionary. \n",
    "ner_results = nlp_roberta(text)\n",
    "entities = combine_tokens_dict(ner_results, entity_classes, stop_words)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the combine_tokens_list function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Ugrasrava', 'entity': 'PER', 'some': 'other'},\n",
       " {'name': 'Lomaharshana', 'entity': 'PER', 'some': 'other'},\n",
       " {'name': 'Sauti', 'entity': 'PER', 'some': 'other'},\n",
       " {'name': 'Puranas', 'entity': 'MISC', 'some': 'other'},\n",
       " {'name': 'Saunaka', 'entity': 'PER', 'some': 'other'},\n",
       " {'name': 'Kulapati', 'entity': 'PER', 'some': 'other'},\n",
       " {'name': 'Naimisha', 'entity': 'LOC', 'some': 'other'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_results = nlp_roberta(text)\n",
    "entities = combine_tokens_list(ner_results, stop_words, metadata = {'some': 'other'})\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a reverse indexed entities dataframe here \n",
    "\n",
    "The idea is to create a dataframe of entities and tag every section they appear in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reverse index\n",
    "## Index all the text chunks that contain a particular named entity. \n",
    "\n",
    "def row2NamedEntities(row):\n",
    "    # print(row)\n",
    "    ner_results = nlp_roberta(row['text'])\n",
    "    entities = combine_tokens_list(ner_results, stop_words, metadata={'chunk_id': row['chunk_id']} )\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfText2DfNE(dataframe):\n",
    "    ## Takes a dataframe from the parsed data and returns dataframe with named entities. \n",
    "    ## The input dataframe must have a text and a chunk_id column. \n",
    "\n",
    "    ## Using swifter for parallelism\n",
    "    ## 1. Calculate named entities for each row of the dataframe. \n",
    "    results = dataframe.swifter.apply(row2NamedEntities, axis=1)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities. \n",
    "    entities_list = np.concatenate(results).ravel().tolist()\n",
    "\n",
    "    ## Remove all NaN entities\n",
    "    entities_dataframe = pd.DataFrame(entities_list).replace(' ', np.nan)\n",
    "    entities_dataframe = entities_dataframe.dropna(subset=['entity'])\n",
    "\n",
    "    ## Count the number of occurances per chunk id\n",
    "    entities_dataframe = entities_dataframe.groupby(['name', 'entity', 'chunk_id']).size().reset_index(name='count')\n",
    "\n",
    "    return entities_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Index each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename : summaries_combined.csv ...\n",
      "Skipping:  summaries_combined.csv\n",
      "Filename : tiny_tales_summaries.csv ...\n",
      "Skipping:  tiny_tales_summaries.csv\n",
      "Filename : km_ganguli_translation_6.csv ...\n",
      "Skipping:  km_ganguli_translation_6.csv\n",
      "Filename : km_ganguli_translation_14.csv ...\n",
      "Skipping:  km_ganguli_translation_14.csv\n",
      "Filename : km_ganguli_translation_15.csv ...\n",
      "Skipping:  km_ganguli_translation_15.csv\n",
      "Filename : km_ganguli_translation_7.csv ...\n",
      "Skipping:  km_ganguli_translation_7.csv\n",
      "Filename : km_ganguli_translation_5.csv ...\n",
      "Skipping:  km_ganguli_translation_5.csv\n",
      "Filename : km_ganguli_translation_17.csv ...\n",
      "Skipping:  km_ganguli_translation_17.csv\n",
      "Filename : km_ganguli_translation_16.csv ...\n",
      "Skipping:  km_ganguli_translation_16.csv\n",
      "Filename : km_ganguli_translation_4.csv ...\n",
      "Skipping:  km_ganguli_translation_4.csv\n",
      "Filename : km_ganguli_translation_12.csv ...\n",
      "Skipping:  km_ganguli_translation_12.csv\n",
      "Filename : km_ganguli_translation_13.csv ...\n",
      "Skipping:  km_ganguli_translation_13.csv\n",
      "Filename : km_ganguli_translation_1.csv ...\n",
      "Skipping:  km_ganguli_translation_1.csv\n",
      "Filename : km_ganguli_translation_3.csv ...\n",
      "Skipping:  km_ganguli_translation_3.csv\n",
      "Filename : tiny_tales_glossary.csv ...\n",
      "Skipping:  tiny_tales_glossary.csv\n",
      "Filename : km_ganguli_translation_11.csv ...\n",
      "Skipping:  km_ganguli_translation_11.csv\n",
      "Filename : km_ganguli_translation_10.csv ...\n",
      "Skipping:  km_ganguli_translation_10.csv\n",
      "Filename : km_ganguli_translation_2.csv ...\n",
      "Skipping:  km_ganguli_translation_2.csv\n",
      "Filename : km_ganguli_translation_9.csv ...\n",
      "Skipping:  km_ganguli_translation_9.csv\n",
      "Filename : kaggle_tilak_summaries.csv ...\n",
      "Skipping:  kaggle_tilak_summaries.csv\n",
      "Filename : km_ganguli_translation_8.csv ...\n",
      "Skipping:  km_ganguli_translation_8.csv\n",
      "Filename : km_ganguli_translation_18.csv ...\n",
      "Skipping:  km_ganguli_translation_18.csv\n",
      "Filename : wikipedia_parva_summaries.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948d5dfb48824536a6ac72dd06171f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file : wikipedia_parva_summaries_named_entities.csv\n"
     ]
    }
   ],
   "source": [
    "ner_directory = directory/\"named_entities\"\n",
    "file_list = glob.glob(f\"{directory}/*.csv\")\n",
    "\n",
    "for file in file_list:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    print(\"Filename :\", file_name, '...')\n",
    "    # if file_name in ['tiny_tales_glossary.csv', 'summaries_combined.csv']:\n",
    "    if not file_name in ['wikipedia_parva_summaries.csv']:\n",
    "        print('Skipping: ', file_name)\n",
    "        continue\n",
    "\n",
    "    df_text = pd.read_csv(directory/file_name, sep=\"|\")\n",
    "    df_ne = dfText2DfNE(df_text)\n",
    "    df_ne['file'] = file_name\n",
    "    outfile_name = f\"{file_name.replace('.csv', '_named_entities.csv')}\"\n",
    "    df_ne.to_csv(ner_directory/outfile_name, index=False, sep=\"|\")\n",
    "    print(\"Wrote file :\", outfile_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI@3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
