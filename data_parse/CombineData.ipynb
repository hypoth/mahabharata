{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data and embed \n",
    "\n",
    "Sources:  \n",
    "1. Tinytales\n",
    "2. Wikipedia, and\n",
    "3. Tilaks Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import statistics\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "## Loaders\n",
    "from langchain.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = Path(\"../text\")\n",
    "directory_tilak = Path(\"../text/KaggleTilak/books\")\n",
    "directory_tinytales = Path(\"../text/TinyTales\")\n",
    "directory_wikipedia = Path(\"../text/Wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_number', 'book_name', 'chapter_name', 'title', 'commentary',\n",
       "       'text', 'section_number', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data = pd.read_csv(directory_tilak/'complete_text_lines.csv', sep=\";\")\n",
    "kaggle_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'section_number', 'title', 'chapter_number', 'chapter_name',\n",
       "       'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinytales_data = pd.read_csv(directory_tinytales/'mahabharata_tiny_tales_stories.csv', sep=\";\")\n",
    "tinytales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book', 'source', 'title', 'book_number', 'description', 'text'], dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_data = pd.read_csv(directory_wikipedia/'wikipedia_parva_summary.csv', sep=\";\")\n",
    "\n",
    "## Droping unnecessary columns\n",
    "wikipedia_data.drop(['start_chapter', 'end_chapter'], axis=1, inplace=True)\n",
    "wikipedia_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the dataframes into one bid dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle data dims (2376, 8) \n",
      " TinyTales data dims (200, 6) \n",
      " Wikipedia data dims (19, 6) \n",
      " Final data dims (2595, 11)\n",
      "Final data columns \n",
      " Index(['book_number', 'book_name', 'chapter_name', 'title', 'commentary',\n",
      "       'text', 'section_number', 'source', 'chapter_number', 'book',\n",
      "       'description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([kaggle_data, tinytales_data, wikipedia_data])\n",
    "print(\n",
    "    \"Kaggle data dims\",  kaggle_data.shape, \"\\n\",\n",
    "    \"TinyTales data dims\", tinytales_data.shape, \"\\n\",\n",
    "    \"Wikipedia data dims\", wikipedia_data.shape, \"\\n\",\n",
    "    \"Final data dims\", df_combined.shape)\n",
    "\n",
    "print(\"Final data columns \\n\", df_combined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate tokens for each text row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per row 167.22851637764933\n",
      "Total number of tokens 433958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_name = \"cl100k_base\"\n",
    "encoding = tiktoken.get_encoding(encoder_name)\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "## Calculat the text tokens per row\n",
    "df_combined['num_text_tokens'] = df_combined['text'].apply(num_tokens_from_string)\n",
    "\n",
    "print(\"Average tokens per row\", statistics.mean(df_combined['num_text_tokens']))\n",
    "print(\"Total number of tokens\", sum(df_combined['num_text_tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the final dataframe into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_number        float64\n",
      "book_name           object\n",
      "chapter_name        object\n",
      "title               object\n",
      "commentary          object\n",
      "text                object\n",
      "section_number     float64\n",
      "source              object\n",
      "chapter_number     float64\n",
      "book                object\n",
      "description         object\n",
      "num_text_tokens      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_combined.to_csv(directory/'combined.csv', index=False, sep=\";\")\n",
    "print(df_combined.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedd and persist into PG Vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver=\"psycopg2\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "    database=os.environ[\"PGVECTOR_DATABASE\"],\n",
    "    user=os.environ[\"PGVECTOR_USER\"],\n",
    "    password=os.environ[\"PGVECTOR_PASSWORD\"],\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"mh_embeddings_summaries\"\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=text_embedding_model)\n",
    "\n",
    "store = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe into a loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df_combined, page_content_column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Helper funciton to combine tokens into names\n",
    "def combine_tokens(ner_results):\n",
    "    name = \"\"\n",
    "    entities = []\n",
    "    for res in ner_results:\n",
    "        word = res['word']\n",
    "        if word[0] == \"‚ñÅ\":\n",
    "            if not name == \"\":\n",
    "                entities = entities + [{'name': name, 'entity': entity}]\n",
    "            name = word[1:]\n",
    "            entity = res['entity']\n",
    "        elif not word in [',', \"'\", \".\", 's', \"'\", \";\", \"(\", \")\"]:\n",
    "            name = name + word\n",
    "    \n",
    "    ## append the last name\n",
    "    entities = entities + [{'name': name, 'entity': entity}]\n",
    "    ## Return\n",
    "    return entities\n",
    "\n",
    "## Get names entities\n",
    "def recognise_named_entities(text, pipeline_model):\n",
    "    ner_results = pipeline_model(text)\n",
    "    return ner_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = docs[100].page_content\n",
    "# print(text)\n",
    "\n",
    "text = \"\"\"\n",
    "\"Thou hast heard, O Raja, of the greatly powerful men of vast exertions,\n",
    "spoken of by Vyasa and the wise Narada; men born of great royal families,\n",
    "resplendent with worthy qualities, versed in the science of celestial\n",
    "arms, and in glory emblems of Indra; men who having conquered the world\n",
    "by justice and performed sacrifices with fit offerings (to the\n",
    "Brahmanas), obtained renown in this world and at last succumbed to the\n",
    "sway of time. Such were Saivya; the valiant Maharatha; Srinjaya, great\n",
    "amongst conquerors. Suhotra; Rantideva, and Kakshivanta, great in glory;\n",
    "Valhika, Damana, Saryati, Ajita, and Nala; Viswamitra the destroyer of\n",
    "foes; Amvarisha, great in strength; Marutta, Manu, Ikshaku, Gaya, and\n",
    "Bharata; Rama the son of Dasaratha; Sasavindu, and Bhagiratha;\n",
    "Kritavirya, the greatly fortunate, and Janamejaya too; and Yayati of good\n",
    "deeds who performed sacrifices, being assisted therein by the celestials\n",
    "themselves, and by whose sacrificial altars and stakes this earth with\n",
    "her habited and uninhabited regions hath been marked all over. These\n",
    "twenty-four Rajas were formerly spoken of by the celestial Rishi Narada\n",
    "unto Saivya when much afflicted for the loss of his children. Besides\n",
    "these, other Rajas had gone before, still more powerful than they, mighty\n",
    "charioteers noble in mind, and resplendent with every worthy quality.\n",
    "These were Puru, Kuru, Yadu, Sura and Viswasrawa of great glory; Anuha,\n",
    "Yuvanaswu, Kakutstha, Vikrami, and Raghu; Vijava, Virihorta, Anga, Bhava,\n",
    "Sweta, and Vripadguru; Usinara, Sata-ratha, Kanka, Duliduha, and Druma;\n",
    "Dambhodbhava, Para, Vena, Sagara, Sankriti, and Nimi; Ajeya, Parasu,\n",
    "Pundra, Sambhu, and holy Deva-Vridha; Devahuya, Supratika, and\n",
    "Vrihad-ratha; Mahatsaha, Vinitatma, Sukratu, and Nala, the king of the\n",
    "Nishadas; Satyavrata, Santabhaya, Sumitra, and the chief Subala;\n",
    "Janujangha, Anaranya, Arka, Priyabhritya, Chuchi-vrata, Balabandhu,\n",
    "Nirmardda, Ketusringa, and Brhidbala; Dhrishtaketu, Brihatketu,\n",
    "Driptaketu, and Niramaya; Abikshit, Chapala, Dhurta, Kritbandhu, and\n",
    "Dridhe-shudhi; Mahapurana-sambhavya, Pratyanga, Paraha and Sruti. These,\n",
    "O chief, and other Rajas, we hear enumerated by hundreds and by\n",
    "thousands, and still others by millions, princes of great power and\n",
    "wisdom, quitting very abundant enjoyments met death as thy sons have\n",
    "done! Their heavenly deeds, valour, and generosity, their magnanimity,\n",
    "faith, truth, purity, simplicity and mercy, are published to the world in\n",
    "the records of former times by sacred bards of great learning. Though\n",
    "endued with every noble virtue, these have yielded up their lives. Thy\n",
    "sons were malevolent, inflamed with passion, avaricious, and of very\n",
    "evil-disposition. Thou art versed in the Sastras, O Bharata, and art\n",
    "intelligent and wise; they never sink under misfortunes whose\n",
    "understandings are guided by the Sastras. Thou art acquainted, O prince,\n",
    "with the lenity and severity of fate; this anxiety therefore for the\n",
    "safety of thy children is unbecoming. Moreover, it behoveth thee not to\n",
    "grieve for that which must happen: for who can avert, by his wisdom, the\n",
    "decrees of fate? No one can leave the way marked out for him by\n",
    "Providence. Existence and non-existence, pleasure and pain all have Time\n",
    "for their root. Time createth all things and Time destroyeth all\n",
    "creatures. It is Time that burneth creatures and it is Time that\n",
    "extinguisheth the fire. All states, the good and the evil, in the three\n",
    "worlds, are caused by Time. Time cutteth short all things and createth\n",
    "them anew. Time alone is awake when all things are asleep: indeed, Time\n",
    "is incapable of being overcome. Time passeth over all things without\n",
    "being retarded. Knowing, as thou dost, that all things past and future\n",
    "and all that exist at the present moment, are the offspring of Time, it\n",
    "behoveth thee not to throw away thy reason.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277456901"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Roberta based NER\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"2rtl3/mn-xlm-roberta-base-named-entity\")\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(\"2rtl3/mn-xlm-roberta-base-named-entity\")\n",
    "nlp_roberta = pipeline(\"ner\", model=roberta_model, tokenizer=roberta_tokenizer)\n",
    "roberta_model.num_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = recognise_named_entities(text, nlp_roberta)\n",
    "entities = combine_tokens(ner_results)\n",
    "df_roberta = pd.DataFrame(entities)\n",
    "\n",
    "# print(text)\n",
    "# for entity in entities:\n",
    "#     print(entity)\n",
    "# for res in ner_results:\n",
    "#     print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbert NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11099913"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbert_tokenizer = AutoTokenizer.from_pretrained(\"ArBert/albert-base-v2-finetuned-ner\")\n",
    "arbert_model = AutoModelForTokenClassification.from_pretrained(\"ArBert/albert-base-v2-finetuned-ner\")\n",
    "nlp_arbert = pipeline(\"token-classification\", model=arbert_model, tokenizer=arbert_tokenizer)\n",
    "arbert_model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = recognise_named_entities(text, nlp_arbert)\n",
    "entities = combine_tokens(ner_results)\n",
    "df_arbert = pd.DataFrame(entities)\n",
    "df_arbert = df_arbert.loc[df_arbert['entity'] != 'LABEL_0']\n",
    "\n",
    "# print(text)\n",
    "# for res in ner_results:\n",
    "#     print(res)\n",
    "# for entity in entities:\n",
    "#     print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndicBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
    "# model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
    "\n",
    "# inputs = tokenizer(\"After Abhimanyu's marriage, there was royal festival and everyone was pleased\", return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# outputs.pooler_output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI@3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
